# ğŸ“¦ Mini-Projet ETL Big Data â€“ Kafka, PySpark, MongoDB

## ğŸ‘¨â€ğŸ’» RÃ©alisÃ© par : Said Azough

## ğŸ¯ Objectif
Mettre en place une pipeline ETL temps rÃ©el :
- GÃ©nÃ©rer des Ã©vÃ©nements utilisateur avec **Kafka**
- Traiter et enrichir les donnÃ©es avec **PySpark Streaming**
- Sauvegarder les donnÃ©es dans **MongoDB**
- Analyser et visualiser les donnÃ©es avec **Pandas / Matplotlib**

---

## âš™ï¸ Technologies utilisÃ©es

- Apache Kafka ğŸ“¨
- Apache Spark (PySpark) âš¡
- MongoDB (via Docker) ğŸ—„ï¸
- Python (Kafka Producer + Visualisation)
- Jupyter Notebook

---

## ğŸ“ Structure du projet

```
bigdata-etl-project/
â”œâ”€â”€ docker-compose.yml           # Lancement de Kafka, Zookeeper, MongoDB
â”œâ”€â”€ producer.py                  # GÃ©nÃ¨re 1000 Ã©vÃ©nements Kafka simulÃ©s
â”œâ”€â”€ spark_streaming.py           # Lit les Ã©vÃ©nements Kafka en streaming (console)
â”œâ”€â”€ etl_to_mongodb.py            # Traite & insÃ¨re les donnÃ©es enrichies dans MongoDB
â”œâ”€â”€ check_mongo.py               # VÃ©rifie le contenu MongoDB
â”œâ”€â”€ analysis.ipynb               # Analyse & visualisation des donnÃ©es
â”œâ”€â”€ etl_pipeline_analysis_maroc.ipynb  # Notebook avec analyse spÃ©cifique sur le Maroc ğŸ‡²ğŸ‡¦
â”œâ”€â”€ *.png                        # Graphiques gÃ©nÃ©rÃ©s (matplotlib/seaborn)
â””â”€â”€ README.md                    # Ce fichier ğŸ“„
```

---

## â–¶ï¸ Lancement rapide

1. DÃ©marrer les services :
```bash
docker-compose up -d
```

2. GÃ©nÃ©rer les messages Kafka :
```bash
python producer.py
```

3. Lancer le traitement Spark :
```bash
python etl_to_mongodb.py
```

4. Analyser les rÃ©sultats :
- Lancer `analysis.ipynb` dans Jupyter
- Ou utiliser le notebook Maroc

---

## ğŸ‡²ğŸ‡¦ Bonus
Analyse dÃ©diÃ©e pour les donnÃ©es `location == "MA"` avec graphe + moyenne des achats.

---

## âœ… RÃ©sultat attendu

- Pipeline complet : **Kafka â†’ Spark â†’ MongoDB â†’ Visualisation**
- Graphiques clairs et filtrÃ©s
- Notebook prÃªt Ã  remettre

---

## ğŸ“¬ Contact

Projet rÃ©alisÃ© dans le cadre du cours Big Data.  


---

## âœ… Conclusion

Ce projet nous a permis, de concevoir et mettre en Å“uvre un pipeline ETL temps rÃ©el complet basÃ© sur Kafka, PySpark et MongoDB.  
Nous avons appris Ã  gÃ©rer des flux de donnÃ©es, Ã  les transformer en streaming, et Ã  les exploiter via des visualisations claires.

Travailler en Ã©quipe nous a aidÃ©s Ã  rÃ©partir les tÃ¢ches, Ã  rÃ©soudre des problÃ¨mes techniques et Ã  mieux comprendre les enjeux du Big Data.  
Lâ€™ajout dâ€™une analyse spÃ©cifique au **Maroc** ğŸ‡²ğŸ‡¦ a permis de contextualiser les rÃ©sultats.

Ce projet a Ã©tÃ© une excellente introduction pratique aux architectures Big Data utilisÃ©es en entreprise.
